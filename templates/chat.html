{% extends "base.html" %}

{% block title %}AI Assistant Chat{% endblock %}

{% block content %}
<div class="px-4 py-6 sm:px-0">
    <div class="max-w-4xl mx-auto">
        <!-- Header -->
        <div class="mb-8">
            <h1 class="text-3xl font-bold text-gray-900">
                <i class="fas fa-comments text-ocb-blue mr-3"></i>
                AI Assistant Chat
            </h1>
            <p class="mt-2 text-gray-600">
                Chat with the AI assistant about your department's documents. The assistant can only access information from your department.
            </p>
        </div>

        <!-- Chat Interface -->
        <div class="bg-white shadow sm:rounded-lg">
            <div class="px-4 py-5 sm:p-6">
                <!-- Chat Messages -->
                <div id="chatMessages" class="space-y-4 mb-6 max-h-96 overflow-y-auto border border-gray-200 rounded-lg p-4">
                    {% if messages %}
                        {% for message in messages %}
                        <div class="flex {% if message.type == 'user' %}justify-end{% else %}justify-start{% endif %}">
                            <div class="max-w-xs lg:max-w-md px-4 py-2 rounded-lg {% if message.type == 'user' %}bg-ocb-blue text-white{% else %}bg-gray-100 text-gray-800{% endif %}">
                                {% if message.type == 'assistant' %}
                                <div class="flex items-center mb-1">
                                    <i class="fas fa-robot text-ocb-blue mr-2"></i>
                                    <span class="text-xs font-medium">AI Assistant</span>
                                </div>
                                {% endif %}
                                <p class="text-sm">{{ message.content|safe }}</p>
                            </div>
                        </div>
                        {% endfor %}
                    {% else %}
                    <div class="text-center text-gray-500 py-8">
                        <i class="fas fa-comments text-4xl mb-4"></i>
                        <p>Start a conversation with the AI assistant</p>
                        <p class="text-sm mt-2">Ask questions about your department's documents</p>
                    </div>
                    {% endif %}
                </div>

                <!-- Chat Input Form -->
                <form method="POST" class="space-y-4">
                    <!-- Audio Recording -->
                    <div class="border border-gray-200 rounded-lg p-4">
                        <div class="flex items-center justify-between mb-3">
                            <div class="flex items-center space-x-2">
                                <!-- Fixed initial button styling -->
                                <button type="button" id="recordBtn" onclick="toggleRecording()" 
                                        class="inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md text-white bg-red-600 hover:bg-red-700 transition-colors duration-200">
                                    <i class="fas fa-microphone mr-2" id="recordIcon"></i>
                                    <span id="recordBtnLabel">Start Recording</span>
                                </button>
                                <span id="recordTimer" class="text-sm text-gray-600 font-mono">00:00 / 01:00</span>
                                <div id="recordingStatus" class="text-xs text-gray-500"></div>
                            </div>
                            <div class="text-xs text-gray-500">Max 1 minute</div>
                        </div>
                        <div class="mb-3">
                            <!-- Fixed canvas with proper sizing -->
                            <canvas id="waveformCanvas" width="600" height="64" 
                                    class="w-full bg-gray-50 border border-gray-200 rounded" 
                                    style="height: 64px;"></canvas>
                        </div>
                        <div>
                            <label for="liveTranscript" class="block text-sm font-medium text-gray-700 mb-1">Live Transcription</label>
                            <textarea id="liveTranscript" rows="2" 
                                      class="shadow-sm focus:ring-ocb-blue focus:border-ocb-blue block w-full sm:text-sm border-gray-300 rounded-md" 
                                      placeholder="Transcribed words will appear here in real time..." readonly></textarea>
                        </div>
                    </div>
                    <div class="flex items-center space-x-4">
                        <div class="flex-1">
                            <label for="message" class="sr-only">Message</label>
                            <textarea name="message" id="message" rows="2" 
                                      class="shadow-sm focus:ring-ocb-blue focus:border-ocb-blue block w-full sm:text-sm border-gray-300 rounded-md"
                                      placeholder="Ask a question about your department's documents or use the microphone above..."
                                      required></textarea>
                        </div>
                        <div class="flex flex-col space-y-2">
                            <select name="language" id="language" class="block w-32 pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-ocb-blue focus:border-ocb-blue sm:text-sm rounded-md">
                                <option value="en">English</option>
                                <option value="ar">العربية</option>
                            </select>
                            <button type="submit" class="inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md text-white bg-ocb-blue hover:bg-blue-700 transition-colors duration-200">
                                <i class="fas fa-paper-plane mr-2"></i>
                                Send
                            </button>
                        </div>
                    </div>
                </form>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<!-- Include browser audio processor as fallback -->
<script src="{{ url_for('static', filename='browser_audio_processor.js') }}"></script>
<script>
(function() {
    'use strict';
    
    // Elements
    const recordBtn = document.getElementById('recordBtn');
    const recordBtnLabel = document.getElementById('recordBtnLabel');
    const recordIcon = document.getElementById('recordIcon');
    const recordTimer = document.getElementById('recordTimer');
    const recordingStatus = document.getElementById('recordingStatus');
    const canvas = document.getElementById('waveformCanvas');
    const transcriptEl = document.getElementById('liveTranscript');
    const messageTextarea = document.getElementById('message');
    
    // Initialize fallback audio processor
    let fallbackProcessor = null;
    let whisperHandler = null;
    
    try {
        if (window.AudioProcessor && window.WhisperAudioHandler) {
            fallbackProcessor = new window.AudioProcessor();
            whisperHandler = new window.WhisperAudioHandler();
            console.log('[voice] Fallback audio processor initialized');
        } else {
            console.warn('[voice] Fallback audio processor not available');
        }
    } catch (error) {
        console.error('[voice] Failed to initialize fallback audio processor:', error);
    }

    // Expose a safe default for onclick even if initialization fails early
    window.toggleRecording = async function() {
        console.warn('[voice] toggleRecording called before initialization');
    };

    if (!recordBtn || !recordTimer || !canvas || !transcriptEl) {
        console.error('[voice] Required elements not found');
        return; // leave the noop toggleRecording in place
    }

    const canvasCtx = canvas.getContext('2d');
    let isRecording = false;
    let startTime = 0;
    let timerInterval = null;
    let rafId = null;

    // Media
    let mediaStream = null;
    let mediaRecorder = null;
    let audioContext = null;
    let analyser = null;
    let source = null;
    let dataArray = null;
    let bufferLength = 0;

    // Server session
    let voiceSessionId = null;
    let recordedChunks = [];

    function formatTime(ms) {
        const totalSec = Math.min(60, Math.floor(ms / 1000));
        const mm = String(Math.floor(totalSec / 60)).padStart(2, '0');
        const ss = String(totalSec % 60).padStart(2, '0');
        return `${mm}:${ss} / 01:00`;
    }

    function clearCanvas() {
        if (!canvasCtx) return;
        canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        canvasCtx.fillStyle = '#f9fafb';
        canvasCtx.fillRect(0, 0, canvas.width, canvas.height);
    }

    function resizeCanvas() {
        const rect = canvas.getBoundingClientRect();
        const dpr = window.devicePixelRatio || 1;
        console.log('[voice] resizeCanvas rect.width=', rect.width, 'dpr=', dpr);
        
        // Set actual canvas pixel size
        canvas.width = Math.max(600, Math.floor(rect.width * dpr));
        canvas.height = Math.floor(64 * dpr);
        
        // Reset and scale the drawing context (avoid cumulative scaling)
        canvasCtx.setTransform(1, 0, 0, 1, 0, 0);
        canvasCtx.scale(dpr, dpr);
        
        // CSS display size
        canvas.style.width = rect.width + 'px';
        canvas.style.height = '64px';
        
        clearCanvas();
    }

    function drawWaveform() {
        if (!analyser || !dataArray || !isRecording) return;
        
        try {
            analyser.getByteTimeDomainData(dataArray);

            // Clear canvas
            canvasCtx.fillStyle = '#f9fafb';
            canvasCtx.fillRect(0, 0, canvas.width / (window.devicePixelRatio || 1), canvas.height / (window.devicePixelRatio || 1));

            // Draw waveform
            canvasCtx.lineWidth = 2;
            canvasCtx.strokeStyle = '#1e40af';
            canvasCtx.beginPath();

            const sliceWidth = (canvas.width / (window.devicePixelRatio || 1)) / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = (v * (canvas.height / (window.devicePixelRatio || 1))) / 2;

                if (i === 0) {
                    canvasCtx.moveTo(x, y);
                } else {
                    canvasCtx.lineTo(x, y);
                }

                x += sliceWidth;
            }

            canvasCtx.lineTo(canvas.width / (window.devicePixelRatio || 1), (canvas.height / (window.devicePixelRatio || 1)) / 2);
            canvasCtx.stroke();

            rafId = requestAnimationFrame(drawWaveform);
        } catch (error) {
            console.error('[voice] Waveform drawing error:', error);
        }
    }

    function updateButtonState(recording) {
        if (recording) {
            recordBtn.classList.remove('bg-red-600', 'hover:bg-red-700');
            recordBtn.classList.add('bg-gray-600', 'hover:bg-gray-700');
            recordBtnLabel.textContent = 'Stop Recording';
            recordIcon.classList.remove('fa-microphone');
            recordIcon.classList.add('fa-stop');
            recordingStatus.textContent = 'Recording...';
            recordingStatus.classList.add('text-red-600');
        } else {
            recordBtn.classList.remove('bg-gray-600', 'hover:bg-gray-700');
            recordBtn.classList.add('bg-red-600', 'hover:bg-red-700');
            recordBtnLabel.textContent = 'Start Recording';
            recordIcon.classList.remove('fa-stop');
            recordIcon.classList.add('fa-microphone');
            recordingStatus.textContent = '';
            recordingStatus.classList.remove('text-red-600');
        }
    }

    async function startRecording() {
        try {
            recordBtn.disabled = true;
            recordingStatus.textContent = 'Starting...';
            console.log('[voice] startRecording invoked');
            
            // Check browser support
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                throw new Error('Microphone access not supported by this browser');
            }

            // Start server voice session (if available)
            try {
                const res = await fetch('/voice/start', { method: 'POST' });
                if (res.ok) {
                    const data = await res.json();
                    voiceSessionId = data.session_id;
                    console.log('[voice] session started, id=', voiceSessionId);
                }
            } catch (error) {
                console.log('[voice] session start not available, proceeding locally:', error);
            }

            // Get microphone access
            mediaStream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            console.log('[voice] getUserMedia acquired');

            // Create audio context for visualization
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            
            if (audioContext.state === 'suspended') {
                await audioContext.resume();
                console.log('[voice] audioContext resumed');
            }

            analyser = audioContext.createAnalyser();
            source = audioContext.createMediaStreamSource(mediaStream);
            source.connect(analyser);
            
            analyser.fftSize = 2048;
            // Use fftSize for time-domain buffer length
            bufferLength = analyser.fftSize;
            dataArray = new Uint8Array(bufferLength);
            console.log('[voice] analyser configured, bufferLength=', bufferLength);

            // Setup media recorder
            const options = {};
            if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
                options.mimeType = 'audio/webm;codecs=opus';
            } else if (MediaRecorder.isTypeSupported('audio/webm')) {
                options.mimeType = 'audio/webm';
            }

            // Ensure a supported MIME type; handle Edge/Firefox variations
            try {
                mediaRecorder = new MediaRecorder(mediaStream, options);
            } catch (e) {
                console.warn('[voice] MediaRecorder init failed with options, retrying without:', e);
                mediaRecorder = new MediaRecorder(mediaStream);
            }
            recordedChunks = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data && event.data.size > 0) {
                    recordedChunks.push(event.data);
                    console.log('[voice] chunk size=', event.data.size);
                }
            };
            mediaRecorder.onerror = (e) => console.error('[voice] MediaRecorder error:', e);
            mediaRecorder.onstart = () => console.log('[voice] MediaRecorder started');
            mediaRecorder.onstop = () => console.log('[voice] MediaRecorder stopped');

            // Collect data and upload once on stop
            mediaRecorder.start();
            console.log('[voice] mediaRecorder.start called');

            // Start recording state
            isRecording = true;
            startTime = Date.now();
            updateButtonState(true);
            
            transcriptEl.value = '';
            resizeCanvas();
            drawWaveform();

            // Start timer
            timerInterval = setInterval(async () => {
                const elapsed = Date.now() - startTime;
                recordTimer.textContent = formatTime(elapsed);
                
                if (elapsed >= 60000) {
                    console.log('[voice] auto stop at 60s');
                    await stopRecording();
                }
            }, 100);

            recordBtn.disabled = false;

        } catch (error) {
            console.error('[voice] Failed to start recording:', error);
            recordingStatus.textContent = `Error: ${error.message}`;
            recordingStatus.classList.add('text-red-600');
            
            // Cleanup on error
            await cleanup();
            recordBtn.disabled = false;
        }
    }

    async function stopRecording() {
        if (!isRecording) return;
        
        isRecording = false;
        updateButtonState(false);
        recordingStatus.textContent = 'Processing...';
        console.log('[voice] stopRecording invoked');

        try {
            // Stop media recorder
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                await new Promise((resolve) => {
                    mediaRecorder.onstop = resolve;
                    mediaRecorder.stop();
                });
            }

            // Upload full recording and get transcript in one request (before cleanup)
            try {
                if (!recordedChunks || recordedChunks.length === 0) {
                    throw new Error('no audio captured');
                }
                
                // Try primary transcription method first
                let transcriptionSuccess = false;
                try {
                    // Prefer WAV when ffmpeg is unavailable on server
                    let chosenMime = (mediaRecorder && mediaRecorder.mimeType) ? mediaRecorder.mimeType : '';
                    let ext = 'webm';
                    if (chosenMime.includes('wav')) {
                        ext = 'wav';
                    } else if (chosenMime.includes('webm')) {
                        ext = 'webm';
                    } else if (chosenMime.includes('ogg')) {
                        ext = 'ogg';
                    }
                    // If server lacks ffmpeg, WAV is safest; attempt to convert
                    // Some browsers do not support WAV via MediaRecorder; if not, keep chosenMime
                    if (!chosenMime) {
                        chosenMime = 'audio/webm';
                    }
                    // Convert to WAV in-browser to avoid server ffmpeg/librosa issues
                    const originalBlob = new Blob(recordedChunks, { type: chosenMime });
                    let wavBuffer = null;
                    if (fallbackProcessor) {
                        try {
                            const conv = await fallbackProcessor.blobToWhisperFormat(originalBlob);
                            if (conv && conv.success && conv.audioData) {
                                wavBuffer = conv.audioData;
                                ext = 'wav';
                                chosenMime = 'audio/wav';
                                console.log('[voice] Converted recording to WAV in-browser');
                            }
                        } catch (ce) {
                            console.warn('[voice] In-browser WAV conversion failed, sending original blob', ce);
                        }
                    }

                    const uploadBlob = wavBuffer ? new Blob([wavBuffer], { type: 'audio/wav' }) : originalBlob;
                    const sr = audioContext ? audioContext.sampleRate : '';
                    const formData = new FormData();
                    formData.append('sample_rate_hz', String(sr));
                    formData.append('audio', uploadBlob, `recording.${ext}`);
                    const response = await fetch('/voice/transcribe', { method: 'POST', body: formData });
                    if (response.ok) {
                        const result = await response.json();
                        if (result.transcript) {
                            transcriptEl.value = result.transcript;
                            messageTextarea.value = result.transcript;
                            console.log('[voice] Primary transcription successful:', result.transcript);
                            transcriptionSuccess = true;
                        } else {
                            console.log('[voice] Primary transcription empty; details:', result);
                        }
                    } else {
                        console.error('[voice] Primary /voice/transcribe HTTP error', response.status);
                    }
                } catch (primaryError) {
                    console.error('[voice] Primary transcription failed:', primaryError);
                }
                
                // Fallback to browser audio processor if primary method failed
                if (!transcriptionSuccess && fallbackProcessor && whisperHandler) {
                    try {
                        console.log('[voice] Attempting fallback transcription with browser processor');
                        const blob = new Blob(recordedChunks, { type: 'audio/webm' });
                        const result = await whisperHandler.processRecordedAudio(blob);
                        if (result.success && result.transcript) {
                            transcriptEl.value = result.transcript;
                            messageTextarea.value = result.transcript;
                            console.log('[voice] Fallback transcription successful:', result.transcript);
                            transcriptionSuccess = true;
                        } else {
                            console.error('[voice] Fallback transcription failed:', result.error);
                        }
                    } catch (fallbackError) {
                        console.error('[voice] Fallback transcription error:', fallbackError);
                    }
                }
                
                if (!transcriptionSuccess) {
                    console.warn('[voice] All transcription methods failed');
                    recordingStatus.textContent = 'Transcription failed - please try again';
                }
                
            } catch (err) {
                console.error('[voice] Audio processing error:', err);
                recordingStatus.textContent = 'Audio processing failed';
            }

            await cleanup();

            recordingStatus.textContent = 'Complete';
            setTimeout(() => {
                recordingStatus.textContent = '';
            }, 2000);

        } catch (error) {
            console.error('Error stopping recording:', error);
            recordingStatus.textContent = 'Error stopping recording';
        }
    }

    async function cleanup() {
        // Stop animation
        if (rafId) {
            cancelAnimationFrame(rafId);
            rafId = null;
        }

        // Clear timer
        if (timerInterval) {
            clearInterval(timerInterval);
            timerInterval = null;
        }

        // Close media stream
        if (mediaStream) {
            mediaStream.getTracks().forEach(track => track.stop());
            mediaStream = null;
        }

        // Close audio context
        if (audioContext && audioContext.state !== 'closed') {
            try {
                await audioContext.close();
            } catch (error) {
                console.error('[voice] Error closing audio context:', error);
            }
            audioContext = null;
        }

        // Reset UI
        recordTimer.textContent = '00:00 / 01:00';
        clearCanvas();
        
        // Clean up references
        analyser = null;
        source = null;
        dataArray = null;
        mediaRecorder = null;
    }

    // Global function for button click (override safe default once ready)
    window.toggleRecording = async function() {
        if (isRecording) {
            await stopRecording();
        } else {
            await startRecording();
        }
    };

    // Initialize
    window.addEventListener('resize', resizeCanvas);
    resizeCanvas();
    
    // Test canvas drawing on load
    setTimeout(() => {
        clearCanvas();
        // Draw a test line to verify canvas is working
        canvasCtx.strokeStyle = '#e5e7eb';
        canvasCtx.lineWidth = 1;
        canvasCtx.beginPath();
        canvasCtx.moveTo(0, canvas.height / (window.devicePixelRatio || 1) / 2);
        canvasCtx.lineTo(canvas.width / (window.devicePixelRatio || 1), canvas.height / (window.devicePixelRatio || 1) / 2);
        canvasCtx.stroke();
    }, 100);

})();
</script>
{% endblock %}
